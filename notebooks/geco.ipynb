{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8383a5d2-1ffd-4094-999d-5dc11dfcd8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy as sp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf7e619-4630-44de-889a-abff4592ac99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Geco():\n",
    "    def __init__(self, pred_fn, score_fn, data, continuous):\n",
    "        self.data = data\n",
    "        self.pred_fn = pred_fn\n",
    "        self.continuous = continuous\n",
    "        self.score_fn = score_fn\n",
    "    \n",
    "    \n",
    "    def get_data_params(self, prec=1, tol=1e-4):\n",
    "        # precision, num_features, feature_range, etc.\n",
    "        self.num_features = self.data.shape[1]\n",
    "        self.feature_names = self.data.columns\n",
    "        self.mad = sp.stats.median_abs_deviation(self.data)\n",
    "        self.feature_range = {}\n",
    "        self.precision = {}\n",
    "        for feature in self.data.columns:\n",
    "            if feature in self.continuous:\n",
    "                self.feature_range[feature] = [np.min(self.data[feature]), np.max(self.data[feature])]\n",
    "                if self.data[feature].dtype == 'int':\n",
    "                    self.precision[feature] = 0\n",
    "                else:\n",
    "                    self.precision[feature] = prec\n",
    "                    \n",
    "            else:\n",
    "                self.feature_range[feature] = np.unique(self.data[feature])\n",
    "        \n",
    "        \n",
    "    def do_random_init(self, num_inits, features_to_vary, query, desired_class):\n",
    "        \n",
    "        init = np.zeros((num_inits, self.num_features))\n",
    "        kx = 0\n",
    "        \n",
    "        while kx < num_inits:\n",
    "            one_init = np.zeros(self.num_features)\n",
    "            for jx, feature in enumerate(self.feature_names):\n",
    "                if feature not in features_to_vary:\n",
    "                    one_int[jx] = query_instance[feature]\n",
    "                    continue\n",
    "                if feature in self.continuous:\n",
    "                    one_init[jx] = np.round(np.random.uniform(\n",
    "                            self.feature_range[feature][0], self.feature_range[feature][1]), self.precision[feature])\n",
    "                else:\n",
    "                    one_init[jx] = np.random.choice(self.feature_range[feature])\n",
    "            test = pd.DataFrame(one_init.reshape(1,-1), columns=self.feature_names)\n",
    "            pred = self.pred_fn(test)\n",
    "            if pred == desired_class:\n",
    "                init[kx] = one_init\n",
    "                kx += 1\n",
    "        return (pd.DataFrame(init, columns=self.feature_names))\n",
    "    \n",
    "    def compute_proximity(self, cfs, query):\n",
    "        diff = np.abs(cfs - np.array(query).reshape(1,-1)) / self.mad\n",
    "        return np.sum(diff, axis=1) / np.sum(self.mad)\n",
    "    \n",
    "    def compute_sparsity(self, cfs, query):\n",
    "        sparsity = np.count_nonzero(cfs - np.array(query).reshape(1,-1), axis=1)\n",
    "        return sparsity / len(self.feature_names)\n",
    "    \n",
    "    def compute_yloss(self, cfs, desired_class):\n",
    "        preds = self.score_fn(cfs)[:,desired_class]\n",
    "        loss = -preds\n",
    "        loss[np.where(preds < 0.5)] += 2\n",
    "        loss[np.where(preds >= 0.5)] = 0\n",
    "        return loss.flatten()\n",
    "    \n",
    "    def compute_loss(self, cfs, query, desired_class):\n",
    "        prox = self.compute_proximity(cfs, query)\n",
    "        sparse = self.compute_sparsity(cfs, query)\n",
    "        yloss = self.compute_yloss(cfs, desired_class)\n",
    "        loss = np.array(prox + sparse + yloss).reshape(-1,1)\n",
    "        index = np.arange(len(cfs)).reshape(-1,1)\n",
    "        return np.concatenate([index, loss], axis=1)\n",
    "    \n",
    "    def mate(self, parent1, parent2, features_to_vary, query):\n",
    "        one_init = np.zeros(self.num_features)\n",
    "        for i in range(self.num_features):\n",
    "            feat = self.feature_names[i]\n",
    "            if feat not in features_to_vary:\n",
    "                one_init[i] = query[feat]\n",
    "                continue\n",
    "                \n",
    "            prob = random.random()\n",
    "            \n",
    "            if prob < 0.40:\n",
    "                one_init[i] = parent1[feat]\n",
    "                \n",
    "            elif prob < 0.80:\n",
    "                one_init[i] = parent2[feat]\n",
    "            \n",
    "            else:\n",
    "                if feat in self.continuous:\n",
    "                    one_init[i] = np.round(\n",
    "                        np.random.uniform(\n",
    "                            self.feature_range[feat][0],\n",
    "                            self.feature_range[feat][1]),\n",
    "                            decimals = self.precision[feat]\n",
    "                    )\n",
    "                else:\n",
    "                    one_init[i] = np.random.choice(\n",
    "                        self.feature_range[self.feat_name[i]]\n",
    "                    )\n",
    "        return one_init\n",
    "    \n",
    "    def get_counterfactuals(self, query, features_to_vary, n_cfs, init_multiplier = 10, maxiter = 500, thres=1e-2):\n",
    "        self.get_data_params()\n",
    "        num_inits = init_multiplier * n_cfs\n",
    "        desired_class = (self.pred_fn(query) + 1) % 2\n",
    "        \n",
    "        population = self.do_random_init(num_inits, features_to_vary, query, desired_class)\n",
    "        iterations = 0\n",
    "        prev_best = -np.inf\n",
    "        cur_best = np.inf\n",
    "        stop_count = 0\n",
    "        cfs_pred = [np.inf] * n_cfs\n",
    "        \n",
    "        while iterations < maxiter:\n",
    "            if (abs(prev_best - cur_best) <= thres) and (i == desired_pred for i in cfs_pred):\n",
    "                stop_count += 1\n",
    "            else:\n",
    "                stop_count = 0\n",
    "            if stop_count >= 5:\n",
    "                break\n",
    "            \n",
    "            prev_best = cur_best\n",
    "            \n",
    "            fitness = self.compute_loss(population, query, desired_class)\n",
    "            fitness = fitness[fitness[:,1].argsort()]\n",
    "            cur_best = fitness[0][1]\n",
    "            \n",
    "            new_generation_1 = population.iloc[fitness[:n_cfs,0], :]\n",
    "            cfs_pred = self.pred_fn(new_generation_1)\n",
    "            \n",
    "            remaining_pop = num_inits - n_cfs\n",
    "            new_generation_2 = np.zeros((remaining_pop, self.num_features))\n",
    "            top_half = fitness[:int(fitness.shape[0]/2),0].astype(np.int32)\n",
    "            for idx in range(remaining_pop):\n",
    "                parent1 = population.iloc[random.choice(top_half),:]\n",
    "                parent2 = population.iloc[random.choice(top_half),:]\n",
    "                child = self.mate(parent1, parent2, features_to_vary, query)\n",
    "                new_generation_2[idx] = child\n",
    "            new_generation_2 = pd.DataFrame(new_generation_2, columns=self.feature_names)\n",
    "            population = pd.concat([new_generation_1, new_generation_2])\n",
    "            \n",
    "            iterations += 1\n",
    "        \n",
    "        return population, fitness, iterations\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29cd145a-269c-4fa1-81d1-a28a8a4f1b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load = datasets.load_breast_cancer(as_frame=True)\n",
    "data = load['data']\n",
    "target = load['target']\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model = model.fit(data, target)\n",
    "continuous = data.select_dtypes(include = 'number').columns\n",
    "query = data.iloc[:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4175b3f1-488d-46c6-b3f9-ae1553cf71a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "continuous = data.select_dtypes(include = 'number').columns\n",
    "query = data.iloc[:1,:]\n",
    "g = Geco(model.predict, model.predict_proba, data, continuous = continuous)\n",
    "test = g.get_counterfactuals(query, features_to_vary=continuous, n_cfs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfe",
   "language": "python",
   "name": "cfe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
